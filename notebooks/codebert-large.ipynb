{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/commit-t5/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"SEBIS/code_trans_t5_large_commit_generation_transfer_learning_finetune\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text):\n",
    "  input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "  output = model.generate(input_ids, max_length=1024)\n",
    "  return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.8 s, sys: 364 ms, total: 29.1 s\n",
      "Wall time: 2.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Add link to react -0.3. 0.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_text = \"\"\"\n",
    "diff --git a/README.md b/README.md\n",
    "index 9b077d6..c9ee84c 100644\n",
    "--- a/README.md\n",
    "+++ b/README.md\n",
    "@@ -6,7 +6,7 @@ React is a JavaScript library for building user interfaces.\n",
    " * **Efficient:** React minimizes interactions with the DOM by using a mock representation of the DOM.\n",
    " * **Flexible:** React works with the libraries and frameworks that you already know.\n",
    " \n",
    "-[Learn how to use React in your own project.](http://facebook.github.io/docs/getting-started.html)\n",
    "+[Learn how to use React in your own project.](http://facebook.github.io/react/docs/getting-started.html)\n",
    " \n",
    " ## Examples\n",
    " \n",
    "@@ -41,7 +41,7 @@ The fastest way to get started is to serve JavaScript from the CDN:\n",
    " <script src=\"http://fb.me/JSXTransformer-0.3.0.js\"></script>\n",
    " ```\n",
    " \n",
    "-We've also built a [starter kit](#) which might be useful if this is your first time using React. It includes a webpage with an example of using React with live code.\n",
    "+We've also built a [starter kit](http://facebook.github.io/react/downloads/react-0.3.0.zip) which might be useful if this is your first time using React. It includes a webpage with an example of using React with live code.\n",
    " \n",
    " If you'd like to use [bower](http://bower.io), it's as easy as:\n",
    "\"\"\"\n",
    "generate(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "t5 doesn't support feature causal-lm. Supported values are: {'default': functools.partial(<bound method OnnxConfig.from_model_config of <class 'transformers.models.t5.configuration_t5.T5OnnxConfig'>>, task='default'), 'default-with-past': functools.partial(<bound method OnnxConfigWithPast.with_past of <class 'transformers.models.t5.configuration_t5.T5OnnxConfig'>>, task='default'), 'seq2seq-lm': functools.partial(<bound method OnnxConfig.from_model_config of <class 'transformers.models.t5.configuration_t5.T5OnnxConfig'>>, task='seq2seq-lm'), 'seq2seq-lm-with-past': functools.partial(<bound method OnnxConfigWithPast.with_past of <class 'transformers.models.t5.configuration_t5.T5OnnxConfig'>>, task='seq2seq-lm')}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m FeaturesManager\n\u001b[0;32m----> 5\u001b[0m model_kind, model_onnx_config \u001b[39m=\u001b[39m FeaturesManager\u001b[39m.\u001b[39;49mcheck_supported_model_or_raise(model, feature\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcausal-lm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m onnx_config \u001b[39m=\u001b[39m model_onnx_config(model\u001b[39m.\u001b[39mconfig)\n\u001b[1;32m      8\u001b[0m \u001b[39m# export\u001b[39;00m\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/transformers/onnx/features.py:730\u001b[0m, in \u001b[0;36mFeaturesManager.check_supported_model_or_raise\u001b[0;34m(model, feature)\u001b[0m\n\u001b[1;32m    728\u001b[0m model_features \u001b[39m=\u001b[39m FeaturesManager\u001b[39m.\u001b[39mget_supported_features_for_model_type(model_type, model_name\u001b[39m=\u001b[39mmodel_name)\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m feature \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_features:\n\u001b[0;32m--> 730\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    731\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt support feature \u001b[39m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m. Supported values are: \u001b[39m\u001b[39m{\u001b[39;00mmodel_features\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel_type, FeaturesManager\u001b[39m.\u001b[39m_SUPPORTED_MODEL_TYPE[model_type][feature]\n",
      "\u001b[0;31mValueError\u001b[0m: t5 doesn't support feature causal-lm. Supported values are: {'default': functools.partial(<bound method OnnxConfig.from_model_config of <class 'transformers.models.t5.configuration_t5.T5OnnxConfig'>>, task='default'), 'default-with-past': functools.partial(<bound method OnnxConfigWithPast.with_past of <class 'transformers.models.t5.configuration_t5.T5OnnxConfig'>>, task='default'), 'seq2seq-lm': functools.partial(<bound method OnnxConfig.from_model_config of <class 'transformers.models.t5.configuration_t5.T5OnnxConfig'>>, task='seq2seq-lm'), 'seq2seq-lm-with-past': functools.partial(<bound method OnnxConfigWithPast.with_past of <class 'transformers.models.t5.configuration_t5.T5OnnxConfig'>>, task='seq2seq-lm')}"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=\"causal-lm\")\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "        preprocessor=tokenizer,\n",
    "        model=model,\n",
    "        config=onnx_config,\n",
    "        opset=13,\n",
    "        output=Path(\"ct-base.onnx\")\n",
    ")\n",
    "\n",
    "# dummy_model_input = tokenizer.encode(\"This is a sample\", return_tensors=\"pt\")\n",
    "\n",
    "# import torch\n",
    "\n",
    "# torch.onnx.export(\n",
    "#     model, \n",
    "#     tuple(dummy_model_input.values()),\n",
    "#     f=\"ct-torch-base.onnx\",  \n",
    "#     export_params=True,  # store the trained parameter weights inside the model file\n",
    "#     opset_version=12,    # the ONNX version to export the model to\n",
    "#     do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "#     input_names = ['input'],   # the model's input names\n",
    "#     output_names = ['output'], # the model's output names\n",
    "#     dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "#                 'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "# from optimum.onnxruntime import ORTQuantizer\n",
    "\n",
    "onnx_model = ORTModelForCausalLM.load_model(\"../models/ct-base_quantized.onnx\")\n",
    "# onnx_model = ORTQuantizer.from_pretrained(onnx_model)\n",
    "onnx_gen = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, accelerator=\"ort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_generate(input_text):\n",
    "  input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "  output = onnx_model.generate(input_ids, max_length=1024)\n",
    "  return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['accelerator'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# %%timeit\u001b[39;00m\n\u001b[1;32m      3\u001b[0m input_text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mdiff --git a/README.md b/README.md\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mindex 9b077d6..c9ee84c 100644\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m If you\u001b[39m\u001b[39m'\u001b[39m\u001b[39md like to use [bower](http://bower.io), it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms as easy as:\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m onnx_gen(input_text)\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[1;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[1;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[1;32m    170\u001b[0m     ):\n\u001b[1;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1109\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1102\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1103\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         )\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1116\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1115\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1116\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1117\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1014\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1015\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1016\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1231\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[0;32m-> 1231\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m   1233\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m logits_processor \u001b[39m=\u001b[39m logits_processor \u001b[39mif\u001b[39;00m logits_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/commit-t5/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1109\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1110\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1111\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1112\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['accelerator'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "input_text = \"\"\"\n",
    "diff --git a/README.md b/README.md\n",
    "index 9b077d6..c9ee84c 100644\n",
    "--- a/README.md\n",
    "+++ b/README.md\n",
    "@@ -6,7 +6,7 @@ React is a JavaScript library for building user interfaces.\n",
    " * **Efficient:** React minimizes interactions with the DOM by using a mock representation of the DOM.\n",
    " * **Flexible:** React works with the libraries and frameworks that you already know.\n",
    " \n",
    "-[Learn how to use React in your own project.](http://facebook.github.io/docs/getting-started.html)\n",
    "+[Learn how to use React in your own project.](http://facebook.github.io/react/docs/getting-started.html)\n",
    " \n",
    " ## Examples\n",
    " \n",
    "@@ -41,7 +41,7 @@ The fastest way to get started is to serve JavaScript from the CDN:\n",
    " <script src=\"http://fb.me/JSXTransformer-0.3.0.js\"></script>\n",
    " ```\n",
    " \n",
    "-We've also built a [starter kit](#) which might be useful if this is your first time using React. It includes a webpage with an example of using React with live code.\n",
    "+We've also built a [starter kit](http://facebook.github.io/react/downloads/react-0.3.0.zip) which might be useful if this is your first time using React. It includes a webpage with an example of using React with live code.\n",
    " \n",
    " If you'd like to use [bower](http://bower.io), it's as easy as:\n",
    "\"\"\"\n",
    "onnx_gen(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fd25b8444efc7bd375a5f52d7295fc56d12a7b735c42b4d370db3bc31589091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
